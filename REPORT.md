
# №43: Exploration Metrics for RL agents.

Авторы: ***Нестерова Мария** (ФИЦ ИУ РАН, аспирант МФТИ), **Загайнов Никита** (Innopolis University), **Манжос Геннадий** (Приволжский исследовательский медицинский университет, аспирант университета ИТМО)*

##


В контексте глубокого обучения с подкреплением (Reinforcement Learning, RL) для решения некоторых задач агенту важно тщательно исследовать среду. Существует множество известных методов для выполнения этой задачи, однако чаще всего сравнение между ними проводится только на основе получаемой награды. Мы предполагаем, что более полезным будет сравнение алгоритмов по степени исследования агентом среды. Сложность заключается в отсутствии единого подхода к количественной оценке исследования для сред с непрерывным пространством состояний или с наблюдениями в виде изображения.

**Цель:** определить несколько метрик для оценки качества исследования агентом среды.

**Задачи:**
- Провести поиск существующих метрик.
- Протестировать метрики на наборе базовых алгоритмов (PPO, SAC и тп) в т.ч. исследования (eps-greedy, entropy, RND, etc), определить зависимость успешности агента от его исследовательских характеристик.
- Определить для метрик их преимущества и недостатки, обозначить область применимости.


## Методы
Как базовую метрику, с которой можно сравниваться, была выбрана метрика **State Counting** (подсчет посещенных состояний).
Дополнительно, было выдвинуто предположение, что метрикой может выступать оценка новизны нового состояния в методах исследования, таких как **RND, Model Disagreement**. С другой стороны, мы предположили, что наиболее общей метрикой буде та, которая способна построить свою оценку на наборе истории состояний после обучения агента. Похожий подход был представлен в статье **Offline Measures for Exploration**. Ниже рассмотрены описанные выше метрики.


**State Counting** - метод, который считает общее кол-во посещенных агентом клеток в заданной среде и строим частоту посещений определенных клеток в виде тепловой карты.
![state-counting](public/state-counting.png)
 https://www.sciencedirect.com/science/article/pii/S1566253522000288?via%3Dihub#b14

**RND** - метод, который вычисляет новизну каждого нового наблюдения с помощью нейронных сетей
![rnd](public/rnd.png)
 [https://arxiv.org/pdf/1810.12894](https://arxiv.org/pdf/1810.12894)

**Model Disagreement** - метод, который оценивает новизну каждого состояния путем подсчета дисперсии предсказаний нескольких моделей 
![model-disagreement](public/model-disagreement.png)
[https://pathak22.github.io/exploration-by-disagreement/resources/icml19.pdf](https://pathak22.github.io/exploration-by-disagreement/resources/icml19.pdf)

**Offline Measures for Exploration** - это офлайн-оценка того, насколько близко распределение собранных состояний к равномерному распределению.
![dist-metric](public/dist-metric.png)
[https://arxiv.org/pdf/2010.15533](https://arxiv.org/pdf/2010.15533)

## Разработка

Для обучения агента был использован метод **PPO**. Мы использовали фреймворк [**cleanRL**](https://docs.cleanrl.dev/) для создания основного пайплайна быстрой воспроизводимости результатов эксперимента, [**tensorboard**](https://www.tensorflow.org/tensorboard) и [**wandb**](https://wandb.ai/site) для трекинга графиков экспериментов. 
Все метрики были реализованы нами, на основе информации и статей.

### Среды
В данной работе мы исследовали существующие метрики для различных популярных сред в обучении с подкреплением, используя фреймворк gymnasium для выбора сред, таких как [**PointMaze**](https://robotics.farama.org/envs/maze/point_maze), [**AntMaze**](https://robotics.farama.org/envs/maze/ant_maze), [**MontezumaRevenge**](https://gymnasium.farama.org/environments/atari/montezuma_revenge/). 

Для создания пайплайна всех экспериментов, мы использовали код алгоритма **PPO**, дополнив в нём соответствующие параметры запуска сред с нужными нам флагами: выбор нужной среды (pointmaze, antmaze, montezuma revenge), выбор размера карт с параметрами small (7x10) | medium (15x15) | large (25x25). 

Таким образом, были созданы кастомные карты сред-лабиринтов в виде матриц, с явно заданными целями **"g" (goal)** и **"r" (agent)**. 

Отметим, что среды PointMaze и AntMaze были выбраны так как в них легко внедрить метрику подсчета состояний (State Counting). Среда montezuma revenge является классической для проверки методов нацеленных на исследование среды.

#### PointMaze
![point-maze-gif](public/point-maze.gif)

#### AntMaze
![ant-maze-gif](public/ant-maze.gif)

#### MontezumaRevenge
![montezume-revenge-gif](public/montezuma-revenge.gif)

## Эксперименты

Все скрипты для запуска обучения агента перечислены в [README](README.md).

### PointMaze
Сперва мы запустили пайплайн со средой **PointMaze**, получив следующие результаты: 

![point-maze](public/point-maze.png)
В лабиринтах малого масштаба метрики, основанные на новизне, дают результаты, аналогичные подходу, основанному на подсчете. Однако наша гипотеза не подтверждается лабиринтами среднего и большого размера, метрики которых дают разные показатели.

### AntMaze
Затем, мы запустили тот же пайплайн со средой **AntMaze**, получив следующие результаты:

![ant-maze](public/ant-maze.png)
Эта среда намного сложнее, поэтому результаты отличаются от результатов выше. Диаграммы ясно показывают сходство результатов для метрики подсчета состояний и метрики **RND**, но метрика **Model Disagreement** с ними не сходится.

### MontezumaRevenge
В конце, мы запустили тот же пайплайн со средой **MontezumaRevenge**, получив следующие результаты:
![montezuma-revenge](public/montezuma-revenge.png)
Для непрерывной среды мы не можем использовать метрику подсчета состояний, поэтому мы используем продолжительность эпизода как непрямую оценку качества исследования основной истины. Диаграммы ясно показывают сходимость метрики **Model Disagreement**, но оценки RND не подтверждают эту закономерность.

### Offline Measures for Exploration
В последние минуты мы запустили оффлайн-тестирование метрики на основе распределений на большой карте **PointMaze**. 

Мы представили три графика с разной настройкой параметров метрики. Результаты:

![dist-point-maze](public/dist-point-maze.png)

Эта метрика имеет схожее поведение с State Counting, она возрастает со временем, что соответствует исследованию агентом пространства со временем. Однако для этой среды по метрике State Counting лучше всего себя показывает метод со внутренней наградой RND, а по оффлайн метрике этот метод показал самый низкий результат при любом способе подсчета.

## Результаты

По итогам проведенной работы, можно сказать о том, что описанные здесь подходы при выборе разных сред показывают что подходы, основанные на оценке новизны, иногда обеспечивают достойную оценку исследования агента RL, но они недостаточно подтверждены, чтобы обобщить этот ответ на все среды и алгоритмы. Что касается distribution-based подходов, теоретические исследования показывают большие перспективы, но они еще не подтверждены на практике. 
